{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-REj_ZUqQSg6",
        "Cos2ieOrSew6",
        "Kyi-dBu26czb",
        "FxFH8Umg_z6F"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this preliminary section, we clustered the papers using HDBSCAN-KNN Algorithm"
      ],
      "metadata": {
        "id": "xvG853X1P1Ox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: HDBSCAN-KNN for clustering"
      ],
      "metadata": {
        "id": "-REj_ZUqQSg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hdbscan, re, gc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "hnuU6VJsQZDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURATION ---\n",
        "COORD_FILE = '/content/drive/MyDrive/100-5D_UMAP_Coordinates.pkl'\n",
        "MAPPING_FILE = '/content/drive/MyDrive/research_map_cooordinates_for_mapping_africa.csv' # File containing umap_2d_x, umap_2d_y\n",
        "TARGET_DIM = 'coords_30d'\n",
        "MIN_CLUSTER_SIZE = 10\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "nknbcGt5Qal4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lzc4YJx8QdFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_coords = pd.read_pickle(COORD_FILE)\n",
        "df_map = pd.read_csv(MAPPING_FILE)\n",
        "\n",
        "df = pd.merge(df_coords, df_map, on='EID')\n",
        "X_30d = np.vstack(df[TARGET_DIM].values).astype('float32') # Extract 30D coordinates into a matrix for HDBSCAN/kNN\n",
        "\n",
        "df['umapX'] = df['umap_2d_x']\n",
        "df['umapY'] = df['umap_2d_y']"
      ],
      "metadata": {
        "id": "lSLe29_UQf0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running HDBSCAN\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=10, gen_min_span_tree=True)\n",
        "labels = clusterer.fit_predict(X_30d)\n",
        "df['original_label'] = labels\n",
        "\n",
        "print(\"\\nInitial Cluster Summary (Raw HDBSCAN Output):\")\n",
        "cluster_counts = df['original_label'].value_counts().sort_index()\n",
        "\n",
        "print(f\"{'Cluster ID':<15} | {'Member Count':<15} | {'Sample EIDs'}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for cid, count in cluster_counts.items():\n",
        "    sample_eids = df[df['original_label'] == cid]['EID'].head(5).tolist()\n",
        "    sample_str = \", \".join(sample_eids)\n",
        "\n",
        "    if cid == -1:\n",
        "        label_name = \"Noise (-1)\"\n",
        "    else:\n",
        "        label_name = f\"Cluster {cid}\"\n",
        "\n",
        "    print(f\"{label_name:<15} | {count:<15} | {sample_str}...\")\n",
        "\n",
        "print(\"-\" * 65)\n",
        "print(f\"Total Papers: {len(df)}\")\n",
        "print(f\"Initial Noise (-1) Count: {cluster_counts.get(-1, 0)} ({cluster_counts.get(-1, 0)/len(df):.1%})\")\n",
        "print(f\"Number of Core Clusters: {len(cluster_counts[cluster_counts.index != -1])}\")\n",
        "\n",
        "core_mask = (labels != -1)\n",
        "noise_mask = (labels == -1)\n",
        "final_labels = labels.copy()\n",
        "spectrum = [f\"Clust{l}(100%)\" if l != -1 else \"\" for l in labels]"
      ],
      "metadata": {
        "id": "HipVJUJcQues"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if noise_mask.any() and core_mask.any():\n",
        "    print(\"\\n -- Refining Noise points using kNN -- \")\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
        "    knn.fit(X_30d[core_mask], labels[core_mask])\n",
        "\n",
        "    probs = knn.predict_proba(X_30d[noise_mask])\n",
        "    preds = knn.predict(X_30d[noise_mask])\n",
        "    final_labels[noise_mask] = preds\n",
        "\n",
        "    classes = knn.classes_\n",
        "    noise_indices = np.where(noise_mask)[0]\n",
        "    for i, p in enumerate(probs):\n",
        "        top = p.argsort()[-3:][::-1] # select top 3\n",
        "        txt = [f\"Clust{classes[idx]}({p[idx]*100:.0f}%)\" for idx in top if p[idx] >= 0.05]\n",
        "        spectrum[noise_indices[i]] = \", \".join(txt)\n",
        "\n",
        "df['cluster_label'], df['cluster_spectrum'] = final_labels, spectrum\n",
        "\n",
        "print(\" ---> Spectrum Analysis Complete\")"
      ],
      "metadata": {
        "id": "wkQ2X4LcQ-Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Density Calculation & Elbow Detection\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=16).fit(X_30d)\n",
        "dists, _ = nn.kneighbors(X_30d)\n",
        "density = np.sum(1.0 / (dists[:, 1:] + 1e-5), axis=1)\n",
        "df['density_score'] = density #find density\n",
        "\n",
        "def find_knee_y(values):\n",
        "    x = np.arange(len(values)); y = values\n",
        "    line_vec = np.array([x[-1] - x[0], y[-1] - y[0]])\n",
        "    line_vec = line_vec / np.linalg.norm(line_vec)\n",
        "    pts = np.vstack((x - x[0], y - y[0])).T\n",
        "    proj = np.outer(np.dot(pts, line_vec), line_vec)\n",
        "    dist = np.linalg.norm(pts - proj, axis=1)\n",
        "    return np.argmax(dist)\n",
        "\n",
        "if noise_mask.any():\n",
        "    noise_vals = df.loc[noise_mask, 'density_score'].values\n",
        "    sorted_den = np.sort(noise_vals)[::-1]\n",
        "    log_den = np.log(sorted_den + 1e-9)\n",
        "\n",
        "    # Detect elbows for classification\n",
        "    idx1 = find_knee_y(log_den)\n",
        "    idx2 = idx1 + find_knee_y(log_den[idx1:])\n",
        "    t1, t2 = np.exp(log_den[idx1]), np.exp(log_den[idx2])\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(np.arange(len(log_den)), log_den, label='log(density)')\n",
        "    plt.axhline(np.log(t1), color='orange', ls='--', label=f'T1: {t1:.2f}')\n",
        "    plt.axhline(np.log(t2), color='red', ls='--', label=f'T2: {t2:.2f}')\n",
        "    plt.title(\"Elbow Cutoff (30D Density)\")\n",
        "    plt.legend(); plt.show()\n",
        "\n",
        "    df['density_class'] = 'core'\n",
        "    df.loc[noise_mask & (df['density_score'] <= t1), 'density_class'] = 'intermediate'\n",
        "    df.loc[noise_mask & (df['density_score'] <= t2), 'density_class'] = 'outlier'"
      ],
      "metadata": {
        "id": "1m3oE_CRRUuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_spectrum(txt):\n",
        "    items = re.findall(r'(Clust\\d+)\\((\\d+)%\\)', str(txt))\n",
        "    spec = sorted([(k, int(v)) for k, v in items], key=lambda x: -x[1])\n",
        "    if not spec: return \"UNKNOWN\"\n",
        "    if len(spec) == 1 or spec[0][1] >= 85: return f\"SINGLE_{spec[0][0]}\"\n",
        "    (A, a), (B, b) = spec[0], spec[1]\n",
        "    if a >= 70: return f\"{A}_DOM_{B}\"\n",
        "    if abs(a - b) <= 15: return f\"{A}_EQ_{B}\"\n",
        "    return f\"{A}_GT_{B}\""
      ],
      "metadata": {
        "id": "V8RfO8ObRgZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['spectrum_class'] = df['cluster_spectrum'].apply(classify_spectrum)\n",
        "df.loc[df['density_class'] == 'outlier', 'spectrum_class'] = 'OUTLIER'\n",
        "\n",
        "# Export\n",
        "df.to_csv(\"Final_Research_Map_Data.csv\", index=False)"
      ],
      "metadata": {
        "id": "GxZxi0wgRt0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 2: Cluster Name Assignment"
      ],
      "metadata": {
        "id": "hl0cWhUM6Qe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0_Lib install"
      ],
      "metadata": {
        "id": "Cos2ieOrSew6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTPqrt376FYg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_import = pd.read_csv('/content/MUSearch/Clustering/research_map_inheritance_logic.csv') #result from clustering"
      ],
      "metadata": {
        "id": "pAdCdzeR6gJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1_Merge Title Abstract Year with EID\n"
      ],
      "metadata": {
        "id": "Kyi-dBu26czb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "    \"/content/MUSearch/Cleaned Data/2021_cleaned_data.csv\", #paper database files from paper filtering process\n",
        "    \"/content/MUSearch/Cleaned Data/2022_cleaned_data.csv\",\n",
        "    \"/content/MUSearch/Cleaned Data/2023_cleaned_data.csv\",\n",
        "    \"/content/MUSearch/Cleaned Data/2024_cleaned_data.csv\",\n",
        "    \"/content/MUSearch/Cleaned Data/2025_cleaned_data.csv\"\n",
        "]\n",
        "\n",
        "df_list = []\n",
        "\n",
        "for file in files:\n",
        "    df = pd.read_csv(file)\n",
        "    df_list.append(df)\n",
        "\n",
        "merged_cleaned_df = pd.concat(df_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "h-PkYZMTFjh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_cleaned_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7W8pr12kRMb",
        "outputId": "ca5cd6ea-4c5e-422f-d378-3302be48021d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21840, 46)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_cleaned_df['EID'].duplicated().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5HmSQ8616s2",
        "outputId": "c489a5fa-f382-4af2-8a59-d8f2bfe189fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add Title and Year and Abstract into df_import with merge of EID (same) if not, write nan\n",
        "df_import = df_import.merge(merged_cleaned_df[['EID', 'Title', 'Year', 'Abstract','Authors']], on='EID', how='left')"
      ],
      "metadata": {
        "id": "tsq2Pw-okcxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2_Centroid calculation and LLM Implementation"
      ],
      "metadata": {
        "id": "kPJhNJth9O-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previous Euclidean-based clustering (not shown in this coding file) failed for groups with hollow centroids. This group name assignment method introduces **Density-Weighted Centroid** **Ranking** to resolve these structural inconsistencies"
      ],
      "metadata": {
        "id": "pBfUeHUW_HUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------\n",
        "Process:\n",
        "\n",
        "1. In layer 5, summarize 20 papers using density and centroid distance to generate a cluster title and a brief cluster description.\n",
        "\n",
        "2. For the higher-level layer, summarize the cluster names and descriptions based on the titles and names of the child layers below.\""
      ],
      "metadata": {
        "id": "n25SEXynE4lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "df_named_m2 = df_import.copy()\n",
        "client = openai.OpenAI(api_key=\"**input your api key\")\n"
      ],
      "metadata": {
        "id": "CEa8gsM8T2Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#centroid paper weighting and listing\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def get_best_representative_papers(group, top_n=20):\n",
        "\n",
        "    coords = group[['umap_3d_x', 'umap_3d_y', 'umap_3d_z']].values\n",
        "    centroid = coords.mean(axis=0).reshape(1, -1)\n",
        "\n",
        "    distances = cdist(coords, centroid, metric='euclidean').flatten()\n",
        "\n",
        "    # Small distance is good, so we use (1 - normalized_distance)\n",
        "    dist_min, dist_max = distances.min(), distances.max()\n",
        "    if dist_max > dist_min:\n",
        "        norm_dist = (distances - dist_min) / (dist_max - dist_min)\n",
        "    else:\n",
        "        norm_dist = np.zeros_like(distances)\n",
        "\n",
        "    # High density is good\n",
        "    dens_min, dens_max = group['density_score'].min(), group['density_score'].max()\n",
        "    if dens_max > dens_min:\n",
        "        norm_dens = (group['density_score'] - dens_min) / (dens_max - dens_min)\n",
        "    else:\n",
        "        norm_dens = np.zeros_like(distances)\n",
        "\n",
        "    # weight 50% distance, 50% density (addictive model)\n",
        "    group['rep_score'] = (1 - norm_dist) + norm_dens\n",
        "\n",
        "    return group.sort_values('rep_score', ascending=False).head(top_n)['EID'].tolist()\n",
        "\n",
        "\n",
        "\n",
        "cluster_summary_m2 = df_named_m2.groupby('5L_Layer_5_Final').apply(\n",
        "    lambda x: pd.Series({\n",
        "        'top_20_EIDs': get_best_representative_papers(x)\n",
        "    })\n",
        ").reset_index()\n",
        "\n",
        "print(f\"Calculated centroids for {len(cluster_summary_m2)} clusters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyJ_KEuP9ZUN",
        "outputId": "015c498a-a709-4351-9872-52cd1441fba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated centroids for 655 clusters.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1162898292.py:32: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cluster_summary_m2 = df_named_m2.groupby('5L_Layer_5_Final').apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run layer 5 topic name and description\n",
        "\n",
        "def get_cluster_details(abstracts):\n",
        "    context = \"\\n---\\n\".join(abstracts[:20])\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following research paper abstracts from a single cluster:\n",
        "    {context}\n",
        "\n",
        "    Task:\n",
        "    1. Provide a technical,descriptive, academic name for this cluster. (not more than 9 words) that represent the scope of the cluster.\n",
        "    2. Provide a 1-sentence short description summarizing the core niche or methodology.\n",
        "\n",
        "    Return ONLY a JSON object: {{\"name\": \"...\", \"description\": \"...\"}}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            response_format={ \"type\": \"json_object\" },\n",
        "            temperature=0.3\n",
        "        )\n",
        "        return json.loads(response.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        return {\"name\": \"Unknown Cluster\", \"description\": \"Description could not be generated.\"}\n",
        "\n",
        "naming_results = []\n",
        "\n",
        "for idx, row in tqdm(cluster_summary_m2.iterrows(), total=len(cluster_summary_m2), desc=\"Generating Names & Descriptions\"):\n",
        "    c_id = row['5L_Layer_5_Final']\n",
        "    eids = row['top_20_EIDs']\n",
        "\n",
        "    abstract_list = df_import[df_import['EID'].isin(eids)]['Abstract'].dropna().tolist()\n",
        "\n",
        "    if abstract_list:\n",
        "        meta = get_cluster_details(abstract_list)\n",
        "        naming_results.append({\n",
        "            'ID': c_id,\n",
        "            'New_Name': meta['name'],\n",
        "            'Desc': meta['description']\n",
        "        })\n",
        "    time.sleep(1.0)\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(naming_results)\n",
        "name_map = results_df.set_index('ID')['New_Name'].to_dict()\n",
        "desc_map = results_df.set_index('ID')['Desc'].to_dict()\n",
        "\n",
        "df_named_m2['5L_Layer_5_Description'] = df_named_m2['5L_Layer_5_Final'].map(desc_map)\n",
        "df_named_m2['5L_Layer_5_Final'] = df_named_m2['5L_Layer_5_Final'].map(name_map).fillna(df_named_m2['5L_Layer_5_Final'])\n",
        "\n",
        "print(f\"\\n ---- Successfully updated df_named_m2 with names and descriptions for {len(results_df)} clusters. ------- \")"
      ],
      "metadata": {
        "id": "wWhYRJHBCM3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_named_m2.rename(columns={'5L_Layer_5_Description': '5L_Layer_5_Final_Description'}, inplace=True)"
      ],
      "metadata": {
        "id": "EFjaeC_gXgrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save to csv\n",
        "df_named_m2.to_csv('/content/MUSearch/Cluster_named_MU_research_m2.csv', index=False)"
      ],
      "metadata": {
        "id": "jkZHC2rFe9PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3_Assign name for layer 1-4 using title and description from previous layer"
      ],
      "metadata": {
        "id": "FxFH8Umg_z6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_named_m3 = df_import.copy()"
      ],
      "metadata": {
        "id": "9H5aqt_B_8KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_model2_results = pd.read_csv('/content/Cluster_named_MU_research_m2.csv')"
      ],
      "metadata": {
        "id": "Di16NG77ArfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_named_m3.rename(columns={'5L_Layer_5_Final': '5L_Layer_5_Final_id'}, inplace=True)\n",
        "df_named_m3.rename(columns={'5L_Layer_4': '5L_Layer_4_id'}, inplace=True)\n",
        "df_named_m3.rename(columns={'5L_Layer_3': '5L_Layer_3_id'}, inplace=True)\n",
        "df_named_m3.rename(columns={'5L_Layer_2': '5L_Layer_2_id'}, inplace=True)\n",
        "df_named_m3.rename(columns={'5L_Layer_1': '5L_Layer_1_id'}, inplace=True)"
      ],
      "metadata": {
        "id": "NOddkNiQEqwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_named_m3 = df_named_m3.merge(\n",
        "    df_model2_results[['EID', '5L_Layer_5_Final', '5L_Layer_5_Final_Description']],\n",
        "    on='EID',\n",
        "    how='left'\n",
        ")"
      ],
      "metadata": {
        "id": "kAXHF94FA64n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now, we already have summary from layer 5 --> process 4-3-2-1 in order.\n",
        "\n",
        "def get_hybrid_hierarchical_metadata(child_info_text, layer_level):\n",
        "    \"\"\"\n",
        "    Summarizes sub-clusters into a parent umbrella name AND description\n",
        "    using strict taxonomical rules.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are a Senior Technical Editor and Taxonomy Specialist. Your task is to organize a research\n",
        "    hierarchy at the {layer_level} level.\n",
        "\n",
        "    SUB-TOPIC DATA (CHILDREN):\n",
        "    {child_info_text}\n",
        "\n",
        "    TASK:\n",
        "    1. Provide a high-precision academic umbrella name (Max 7 words).\n",
        "       - Use '&' to join primary themes (e.g., 'Vision & 3D').\n",
        "       - Use '( )' for high-signal sub-niches (e.g., 'LLM Evals (RAG, Agents)').\n",
        "       - Use ':' for deep sub-specializations.\n",
        "    2. Provide a 1-2 sentence description summarizing the collective scope,\n",
        "       focusing on methodology or innovative discovery keywords.\n",
        "\n",
        "    NAMING RULES:\n",
        "    - PATTERN: Use 'Technique: Application A & Application B'.\n",
        "    - NO FILLER: Absolutely no \"Advances in\", \"Research on\", \"Methods\", \"Study of\".\n",
        "    - BE SPECIFIC: Use technical anchors (e.g., 'Preference Alignment', 'Inverse Problems').\n",
        "    - DEDUPLICATION: Ensure the name is unique to this specific group of children.\n",
        "\n",
        "    Return ONLY a JSON object: {{\"name\": \"...\", \"description\": \"...\"}}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            response_format={ \"type\": \"json_object\" },\n",
        "            temperature=0.3\n",
        "        )\n",
        "        import json\n",
        "        return json.loads(response.choices[0].message.content)\n",
        "    except Exception as e:\n",
        "        return {\"name\": f\"General {layer_level} Category\", \"description\": \"Combined research area.\"}\n",
        "\n",
        "# Format: (Target_ID_Col, Target_Name_Col, Source_Name_Col, Source_Desc_Col)\n",
        "hierarchy_steps = [\n",
        "    ('5L_Layer_4_id', '5L_Layer_4', '5L_Layer_5_Final', '5L_Layer_5_Final_Description'),\n",
        "    ('5L_Layer_3_id', '5L_Layer_3', '5L_Layer_4', '5L_Layer_4_Description'),\n",
        "    ('5L_Layer_2_id', '5L_Layer_2', '5L_Layer_3', '5L_Layer_3_Description'),\n",
        "    ('5L_Layer_1_id', '5L_Layer_1', '5L_Layer_2', '5L_Layer_2_Description')\n",
        "]\n",
        "\n",
        "for target_id_col, target_name_col, source_name_col, source_desc_col in hierarchy_steps:\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"STEP: Naming & Describing {target_name_col} using {source_name_col}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    parent_groups = df_named_m3.groupby(target_id_col)\n",
        "    name_map = {}\n",
        "    desc_map = {}\n",
        "\n",
        "    for parent_id, group in tqdm(parent_groups, desc=f\"Processing {target_name_col}\"):\n",
        "        if pd.isna(parent_id): continue\n",
        "\n",
        "        unique_children = group.drop_duplicates(subset=[source_name_col])\n",
        "        child_context = [f\"- {row[source_name_col]}: {row[source_desc_col]}\" for _, row in unique_children.iterrows()]\n",
        "        child_info_text = \"\\n\".join(child_context)\n",
        "\n",
        "        meta = get_hybrid_hierarchical_metadata(child_info_text, target_name_col)\n",
        "        name_map[parent_id] = meta['name']\n",
        "        desc_map[parent_id] = meta['description']\n",
        "\n",
        "        time.sleep(0.8)\n",
        "\n",
        "    # Save name and description\n",
        "    df_named_m3[f\"{target_name_col}_Description\"] = df_named_m3[target_id_col].map(desc_map)\n",
        "    df_named_m3[target_name_col] = df_named_m3[target_id_col].map(name_map)\n",
        "\n",
        "print(\"\\n layer 1-4 Naming and Description complete\")"
      ],
      "metadata": {
        "id": "bp8L4JgM3UjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if layer 5 is nan, make sure all description in all layer is nan\n",
        "\n",
        "nan_mask = df_named_m3['5L_Layer_5_Final'].isna()\n",
        "\n",
        "description_cols = [\n",
        "    '5L_Layer_5_Final_Description',\n",
        "    '5L_Layer_4_Description',\n",
        "    '5L_Layer_3_Description',\n",
        "    '5L_Layer_2_Description',\n",
        "    '5L_Layer_1_Description',\n",
        "    '5L_Layer_1',\n",
        "    '5L_Layer_2',\n",
        "    '5L_Layer_3',\n",
        "\n",
        "]\n",
        "df_named_m3.loc[nan_mask, description_cols] = np.nan\n",
        "\n",
        "print(\"Data consistency check and NaN propagation for description columns complete.\")"
      ],
      "metadata": {
        "id": "U_TJFAxL47cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_named_m3['Authors'] = df_import['Authors']"
      ],
      "metadata": {
        "id": "Cxd5k3zYu__-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_named_m3.to_csv('/content/MUSearch/Cluster_named_MU_research_m3.csv', index=False)"
      ],
      "metadata": {
        "id": "7hzXOlXt03hW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
