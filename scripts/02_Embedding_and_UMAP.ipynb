{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embedding and Dimensionality Reduction\n",
    "\n",
    "This notebook generates semantic embeddings from research papers and performs dimensionality reduction:\n",
    "1. **Text Embedding**: Convert paper abstracts to high-dimensional vectors using Cohere API\n",
    "2. **UMAP Reduction**: Reduce embeddings to 2D coordinates for visualization\n",
    "3. **HDBSCAN Clustering**: Group papers by semantic similarity\n",
    "\n",
    "**Input**: Cleaned paper data from previous step  \n",
    "**Output**: Papers with embeddings, UMAP coordinates, and cluster assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cohere\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "# Configuration\n",
    "COHERE_API_KEY = \"your_cohere_api_key_here\"  # Replace with your API key\n",
    "EMBEDDING_MODEL = \"embed-english-v3.0\"\n",
    "BATCH_SIZE = 48\n",
    "SLEEP_TIME = 1.2\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"Embedding Table.csv\"\n",
    "OUTPUT_FILE = \"papers_with_embeddings.pkl\"\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(COHERE_API_KEY)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    test_response = co.embed(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        texts=[\"test sentence\"],\n",
    "        input_type=\"search_document\"\n",
    "    )\n",
    "    print(f\"‚úÖ Cohere API connected. Embedding dimension: {len(test_response.embeddings[0])}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå API connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned paper data\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"Loaded dataset: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Prepare texts for embedding\n",
    "texts = df[\"Embedding Text\"].astype(str).tolist()\n",
    "print(f\"Ready to embed {len(texts)} texts\")\n",
    "\n",
    "# Display sample text\n",
    "print(f\"\\nSample text: {texts[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Text Embeddings\n",
    "\n",
    "This process converts text to high-dimensional vectors with robust error handling and progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_with_resume(texts, save_file):\n",
    "    \"\"\"Generate embeddings with ability to resume from interruptions\"\"\"\n",
    "    \n",
    "    # Check for existing progress\n",
    "    if os.path.exists(save_file):\n",
    "        df_saved = pd.read_pickle(save_file)\n",
    "        existing_embeddings = df_saved[\"embedding\"].tolist()\n",
    "        print(f\"üìÑ Resuming from {len(existing_embeddings)} existing embeddings\")\n",
    "    else:\n",
    "        existing_embeddings = []\n",
    "        print(\"üÜï Starting fresh embedding generation\")\n",
    "    \n",
    "    start_idx = len(existing_embeddings)\n",
    "    all_embeddings = existing_embeddings.copy()\n",
    "    \n",
    "    # Process remaining texts in batches\n",
    "    for i in tqdm(range(start_idx, len(texts), BATCH_SIZE), \n",
    "                  desc=\"Generating embeddings\",\n",
    "                  initial=start_idx//BATCH_SIZE):\n",
    "        \n",
    "        batch = texts[i:i + BATCH_SIZE]\n",
    "        retries = 0\n",
    "        \n",
    "        while retries < MAX_RETRIES:\n",
    "            try:\n",
    "                # Generate embeddings for batch\n",
    "                response = co.embed(\n",
    "                    model=EMBEDDING_MODEL,\n",
    "                    texts=batch,\n",
    "                    input_type=\"search_document\"\n",
    "                )\n",
    "                \n",
    "                # Add to collection\n",
    "                all_embeddings.extend(response.embeddings)\n",
    "                \n",
    "                # Save progress incrementally\n",
    "                df_partial = pd.DataFrame({\n",
    "                    \"EID\": df[\"EID\"][:len(all_embeddings)],\n",
    "                    \"Year\": df[\"Year\"][:len(all_embeddings)],\n",
    "                    \"Embedding Text\": df[\"Embedding Text\"][:len(all_embeddings)],\n",
    "                    \"embedding\": all_embeddings\n",
    "                })\n",
    "                df_partial.to_pickle(save_file)\n",
    "                \n",
    "                time.sleep(SLEEP_TIME)\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                wait_time = 10 * retries\n",
    "                print(f\"\\n‚ö†Ô∏è  Batch {i} failed (attempt {retries}/{MAX_RETRIES}): {e}\")\n",
    "                \n",
    "                if retries < MAX_RETRIES:\n",
    "                    print(f\"‚è≥ Waiting {wait_time}s before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"‚ùå Skipping batch {i} after {MAX_RETRIES} failures\")\n",
    "    \n",
    "    return pd.read_pickle(save_file)\n",
    "\n",
    "# Generate embeddings\n",
    "df_with_embeddings = generate_embeddings_with_resume(texts, OUTPUT_FILE)\n",
    "print(f\"\\n‚úÖ Embedding generation complete!\")\n",
    "print(f\"üìä Dataset shape: {df_with_embeddings.shape}\")\n",
    "print(f\"üßÆ Embedding dimension: {len(df_with_embeddings['embedding'][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction with UMAP\n",
    "\n",
    "Create 2D coordinates for visualization and reduced dimensions for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix\n",
    "X = np.vstack(df_with_embeddings[\"embedding\"].values)\n",
    "print(f\"Embedding matrix shape: {X.shape}\")\n",
    "\n",
    "# UMAP for clustering (higher dimensions)\n",
    "print(\"üîÑ Reducing dimensions for clustering...\")\n",
    "umap_cluster = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=10,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42\n",
    ")\n",
    "X_cluster = umap_cluster.fit_transform(X)\n",
    "print(f\"Cluster embedding shape: {X_cluster.shape}\")\n",
    "\n",
    "# UMAP for visualization (2D)\n",
    "print(\"üîÑ Creating 2D visualization coordinates...\")\n",
    "umap_viz = umap.UMAP(\n",
    "    n_neighbors=30,\n",
    "    n_components=2,\n",
    "    min_dist=0.1,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42\n",
    ")\n",
    "X_viz = umap_viz.fit_transform(X)\n",
    "\n",
    "# Add coordinates to dataframe\n",
    "df_with_embeddings[\"umap_x\"] = X_viz[:, 0]\n",
    "df_with_embeddings[\"umap_y\"] = X_viz[:, 1]\n",
    "\n",
    "print(\"‚úÖ UMAP reduction complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Clustering with HDBSCAN\n",
    "\n",
    "Group papers by semantic similarity using the reduced dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform HDBSCAN clustering\n",
    "print(\"üîÑ Performing semantic clustering...\")\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=30,\n",
    "    min_samples=5,\n",
    "    cluster_selection_method=\"eom\",\n",
    "    metric=\"euclidean\"\n",
    ")\n",
    "\n",
    "cluster_labels = clusterer.fit_predict(X_cluster)\n",
    "df_with_embeddings[\"topic\"] = cluster_labels\n",
    "\n",
    "# Analyze clustering results\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "noise_ratio = (cluster_labels == -1).mean()\n",
    "cluster_sizes = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "\n",
    "print(f\"\\nüìä Clustering Results:\")\n",
    "print(f\"   üìå Number of clusters: {n_clusters}\")\n",
    "print(f\"   üîá Noise ratio: {noise_ratio:.1%}\")\n",
    "print(f\"   üìà Largest cluster: {cluster_sizes.iloc[1:].max() if len(cluster_sizes) > 1 else 0} papers\")\n",
    "print(f\"   üìâ Average cluster size: {cluster_sizes.iloc[1:].mean():.1f} papers\")\n",
    "\n",
    "# Show sample from largest cluster\n",
    "if n_clusters > 0:\n",
    "    largest_cluster = cluster_sizes.iloc[1:].idxmax()\n",
    "    sample_texts = df_with_embeddings[df_with_embeddings[\"topic\"] == largest_cluster][\"Embedding Text\"].head(3)\n",
    "    print(f\"\\nüìù Sample from cluster {largest_cluster}:\")\n",
    "    for i, text in enumerate(sample_texts, 1):\n",
    "        print(f\"   {i}. {text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete dataset with all features\n",
    "df_with_embeddings.to_pickle(\"research_with_topics_umap.pkl\")\n",
    "print(\"‚úÖ Saved complete dataset with embeddings\")\n",
    "\n",
    "# Save visualization-ready dataset (without embeddings for smaller file size)\n",
    "df_viz = df_with_embeddings.drop(columns=[\"embedding\"])\n",
    "df_viz.to_csv(\"research_map_ready.csv\", index=False)\n",
    "print(\"‚úÖ Saved visualization-ready CSV\")\n",
    "\n",
    "print(f\"\\nüìã Final dataset summary:\")\n",
    "print(f\"   üìÑ Total papers: {len(df_with_embeddings)}\")\n",
    "print(f\"   üè∑Ô∏è  Clustered papers: {len(df_with_embeddings[df_with_embeddings['topic'] != -1])}\")\n",
    "print(f\"   üîç Ready for visualization and analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
