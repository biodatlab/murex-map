{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Clustering and Semantic Naming\n",
    "\n",
    "This notebook performs sophisticated clustering and generates meaningful cluster names:\n",
    "1. **HDBSCAN-kNN Clustering**: Advanced clustering with noise refinement\n",
    "2. **Density Analysis**: Classify points by local density patterns\n",
    "3. **LLM-Based Naming**: Generate descriptive cluster names using GPT-4o-mini\n",
    "4. **Hierarchical Organization**: Create multi-level cluster taxonomy\n",
    "\n",
    "**Input**: Papers with embeddings and initial clustering  \n",
    "**Output**: Named clusters organized in hierarchical structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "from scipy.spatial.distance import cdist\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "COORD_FILE = '/content/drive/MyDrive/100-5D_UMAP_Coordinates.pkl'\n",
    "MAPPING_FILE = '/content/drive/MyDrive/research_map_coordinates_for_mapping_africa.csv'\n",
    "TARGET_DIM = 'coords_30d'\n",
    "MIN_CLUSTER_SIZE = 10\n",
    "OPENAI_API_KEY = \"**input your api key\"\n",
    "SEED = 42\n",
    "\n",
    "# Load datasets\n",
    "df_coords = pd.read_pickle(COORD_FILE)\n",
    "df_map = pd.read_csv(MAPPING_FILE)\n",
    "df = pd.merge(df_coords, df_map, on='EID')\n",
    "\n",
    "# Prepare coordinates\n",
    "X_30d = np.vstack(df[TARGET_DIM].values).astype('float32')\n",
    "df['umapX'] = df['umap_2d_x']\n",
    "df['umapY'] = df['umap_2d_y']\n",
    "\n",
    "print(f\"üìä Dataset loaded: {df.shape}\")\n",
    "print(f\"üßÆ Coordinate matrix: {X_30d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HDBSCAN-kNN Clustering\n",
    "\n",
    "Advanced clustering that refines noise points using k-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_hdbscan_knn_clustering(X, min_cluster_size=10):\n",
    "    \"\"\"Perform HDBSCAN clustering with kNN refinement\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Running initial HDBSCAN clustering...\")\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        gen_min_span_tree=True\n",
    "    )\n",
    "    initial_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    # Analyze initial results\n",
    "    core_mask = (initial_labels != -1)\n",
    "    noise_mask = (initial_labels == -1)\n",
    "    n_clusters = len(set(initial_labels)) - (1 if -1 in initial_labels else 0)\n",
    "    noise_ratio = noise_mask.mean()\n",
    "    \n",
    "    print(f\"üìä Initial clustering results:\")\n",
    "    print(f\"   üè∑Ô∏è  Clusters found: {n_clusters}\")\n",
    "    print(f\"   üîá Noise ratio: {noise_ratio:.1%}\")\n",
    "    \n",
    "    # Refine noise points using kNN\n",
    "    final_labels = initial_labels.copy()\n",
    "    cluster_spectrum = [''] * len(initial_labels)\n",
    "    \n",
    "    if noise_mask.any() and core_mask.any():\n",
    "        print(\"üîÑ Refining noise points with k-nearest neighbors...\")\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "        knn.fit(X[core_mask], initial_labels[core_mask])\n",
    "        \n",
    "        # Predict cluster assignments for noise points\n",
    "        noise_predictions = knn.predict(X[noise_mask])\n",
    "        prediction_probs = knn.predict_proba(X[noise_mask])\n",
    "        \n",
    "        final_labels[noise_mask] = noise_predictions\n",
    "        \n",
    "        # Generate confidence spectrum for each point\n",
    "        classes = knn.classes_\n",
    "        noise_indices = np.where(noise_mask)[0]\n",
    "        \n",
    "        for i, probs in enumerate(prediction_probs):\n",
    "            top_3 = probs.argsort()[-3:][::-1]\n",
    "            spectrum_parts = [\n",
    "                f\"Clust{classes[idx]}({probs[idx]*100:.0f}%)\"\n",
    "                for idx in top_3 if probs[idx] >= 0.05\n",
    "            ]\n",
    "            cluster_spectrum[noise_indices[i]] = \", \".join(spectrum_parts)\n",
    "    \n",
    "    # Set spectrum for core points\n",
    "    for i, label in enumerate(initial_labels):\n",
    "        if label != -1:\n",
    "            cluster_spectrum[i] = f\"Clust{label}(100%)\"\n",
    "    \n",
    "    refined_noise_ratio = (final_labels == -1).mean()\n",
    "    print(f\"‚úÖ Refinement complete. Noise reduced to: {refined_noise_ratio:.1%}\")\n",
    "    \n",
    "    return final_labels, cluster_spectrum, initial_labels\n",
    "\n",
    "# Perform clustering\n",
    "cluster_labels, spectrum, original_labels = perform_hdbscan_knn_clustering(X_30d, MIN_CLUSTER_SIZE)\n",
    "\n",
    "df['cluster_label'] = cluster_labels\n",
    "df['cluster_spectrum'] = spectrum\n",
    "df['original_label'] = original_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Density Analysis and Classification\n",
    "\n",
    "Analyze local density patterns to classify points into density-based categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_density_patterns(X, cluster_labels, k=16):\n",
    "    \"\"\"Analyze local density and classify points\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Calculating local density scores...\")\n",
    "    \n",
    "    # Calculate k-nearest neighbor distances\n",
    "    nn = NearestNeighbors(n_neighbors=k).fit(X)\n",
    "    distances, _ = nn.kneighbors(X)\n",
    "    \n",
    "    # Compute density score (inverse of average distance)\n",
    "    density_scores = np.sum(1.0 / (distances[:, 1:] + 1e-5), axis=1)\n",
    "    \n",
    "    # Find density thresholds using knee detection\n",
    "    def find_knee_point(values):\n",
    "        \"\"\"Find the knee point in a sorted curve\"\"\"\n",
    "        x = np.arange(len(values))\n",
    "        y = values\n",
    "        \n",
    "        # Vector from first to last point\n",
    "        line_vec = np.array([x[-1] - x[0], y[-1] - y[0]])\n",
    "        line_vec = line_vec / np.linalg.norm(line_vec)\n",
    "        \n",
    "        # Calculate perpendicular distances\n",
    "        points = np.vstack((x - x[0], y - y[0])).T\n",
    "        projections = np.outer(np.dot(points, line_vec), line_vec)\n",
    "        distances_to_line = np.linalg.norm(points - projections, axis=1)\n",
    "        \n",
    "        return np.argmax(distances_to_line)\n",
    "    \n",
    "    # Analyze noise points only\n",
    "    noise_mask = (cluster_labels == -1)\n",
    "    density_classes = ['core'] * len(cluster_labels)\n",
    "    \n",
    "    if noise_mask.any():\n",
    "        noise_densities = density_scores[noise_mask]\n",
    "        sorted_densities = np.sort(noise_densities)[::-1]\n",
    "        log_densities = np.log(sorted_densities + 1e-9)\n",
    "        \n",
    "        # Find two knee points for classification\n",
    "        knee1_idx = find_knee_point(log_densities)\n",
    "        knee2_idx = knee1_idx + find_knee_point(log_densities[knee1_idx:])\n",
    "        \n",
    "        threshold1 = np.exp(log_densities[knee1_idx])\n",
    "        threshold2 = np.exp(log_densities[knee2_idx])\n",
    "        \n",
    "        print(f\"üìà Density thresholds: T1={threshold1:.2f}, T2={threshold2:.2f}\")\n",
    "        \n",
    "        # Classify density levels\n",
    "        for i, (label, density) in enumerate(zip(cluster_labels, density_scores)):\n",
    "            if label == -1:  # Only classify noise points\n",
    "                if density <= threshold2:\n",
    "                    density_classes[i] = 'outlier'\n",
    "                elif density <= threshold1:\n",
    "                    density_classes[i] = 'intermediate'\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(log_densities, label='Log Density')\n",
    "        plt.axhline(np.log(threshold1), color='orange', linestyle='--', label=f'T1: {threshold1:.2f}')\n",
    "        plt.axhline(np.log(threshold2), color='red', linestyle='--', label=f'T2: {threshold2:.2f}')\n",
    "        plt.title('Density Threshold Detection')\n",
    "        plt.xlabel('Sorted Points')\n",
    "        plt.ylabel('Log Density')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return density_scores, density_classes\n",
    "\n",
    "# Perform density analysis\n",
    "density_scores, density_classes = analyze_density_patterns(X_30d, cluster_labels)\n",
    "\n",
    "df['density_score'] = density_scores\n",
    "df['density_class'] = density_classes\n",
    "\n",
    "print(\"‚úÖ Density analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge Paper Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned paper data with titles and abstracts\n",
    "paper_files = [\n",
    "    \"/content/MUSearch/Cleaned Data/2021_cleaned_data.csv\",\n",
    "    \"/content/MUSearch/Cleaned Data/2022_cleaned_data.csv\",\n",
    "    \"/content/MUSearch/Cleaned Data/2023_cleaned_data.csv\",\n",
    "    \"/content/MUSearch/Cleaned Data/2024_cleaned_data.csv\",\n",
    "    \"/content/MUSearch/Cleaned Data/2025_cleaned_data.csv\"\n",
    "]\n",
    "\n",
    "paper_dataframes = []\n",
    "for file_path in paper_files:\n",
    "    if os.path.exists(file_path):\n",
    "        paper_dataframes.append(pd.read_csv(file_path))\n",
    "\n",
    "if paper_dataframes:\n",
    "    merged_papers = pd.concat(paper_dataframes, ignore_index=True)\n",
    "    \n",
    "    # Merge with clustering results\n",
    "    df = df.merge(\n",
    "        merged_papers[['EID', 'Title', 'Year', 'Abstract', 'Authors']], \n",
    "        on='EID', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"üìÑ Merged paper metadata: {len(merged_papers)} papers\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Paper metadata files not found\")\n",
    "\n",
    "# Save intermediate results\n",
    "df.to_csv(\"clustering_intermediate_results.csv\", index=False)\n",
    "print(\"üíæ Saved intermediate clustering results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Cluster Names with LLM\n",
    "\n",
    "Use GPT-4o-mini to generate descriptive names for each cluster based on representative papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representative_papers(cluster_df, n_papers=20):\n",
    "    \"\"\"Select most representative papers based on centroid distance and density\"\"\"\n",
    "    \n",
    "    if len(cluster_df) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Use 3D coordinates for centroid calculation\n",
    "    coords = cluster_df[['umap_3d_x', 'umap_3d_y', 'umap_3d_z']].values\n",
    "    centroid = coords.mean(axis=0).reshape(1, -1)\n",
    "    \n",
    "    # Calculate distances to centroid\n",
    "    distances = cdist(coords, centroid, metric='euclidean').flatten()\n",
    "    \n",
    "    # Normalize distance and density scores\n",
    "    dist_min, dist_max = distances.min(), distances.max()\n",
    "    norm_distances = (distances - dist_min) / (dist_max - dist_min) if dist_max > dist_min else np.zeros_like(distances)\n",
    "    \n",
    "    dens_min, dens_max = cluster_df['density_score'].min(), cluster_df['density_score'].max()\n",
    "    norm_densities = (cluster_df['density_score'] - dens_min) / (dens_max - dens_min) if dens_max > dens_min else np.zeros_like(distances)\n",
    "    \n",
    "    # Combine scores (closer to centroid + higher density = better representative)\n",
    "    representative_scores = (1 - norm_distances) + norm_densities\n",
    "    \n",
    "    # Select top papers\n",
    "    top_indices = representative_scores.argsort()[-n_papers:][::-1]\n",
    "    return cluster_df.iloc[top_indices]['EID'].tolist()\n",
    "\n",
    "def generate_cluster_name_description(abstracts, client):\n",
    "    \"\"\"Generate cluster name and description using LLM\"\"\"\n",
    "    \n",
    "    context = \"\\n---\\n\".join(abstracts[:20])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze these research paper abstracts from a single cluster:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Generate:\n",
    "    1. A concise, technical cluster name (max 9 words)\n",
    "    2. A one-sentence description of the research scope/methodology\n",
    "    \n",
    "    Return only a JSON object: {{\"name\": \"...\", \"description\": \"...\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"LLM error: {e}\")\n",
    "        return {\"name\": \"Unknown Cluster\", \"description\": \"Description not available.\"}\n",
    "\n",
    "def process_cluster_naming(df):\n",
    "    \"\"\"Generate names for all clusters\"\"\"\n",
    "    \n",
    "    if not OPENAI_API_KEY or OPENAI_API_KEY == \"**input your api key\":\n",
    "        print(\"‚ö†Ô∏è  Please set your OpenAI API key\")\n",
    "        return df\n",
    "    \n",
    "    client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    # Group by cluster\n",
    "    cluster_groups = df.groupby('5L_Layer_5_Final')\n",
    "    naming_results = []\n",
    "    \n",
    "    print(f\"üè∑Ô∏è  Processing {len(cluster_groups)} clusters for naming...\")\n",
    "    \n",
    "    for cluster_id, cluster_group in tqdm(cluster_groups, desc=\"Naming clusters\"):\n",
    "        if pd.isna(cluster_id):\n",
    "            continue\n",
    "        \n",
    "        # Get representative papers\n",
    "        representative_eids = get_representative_papers(cluster_group)\n",
    "        \n",
    "        # Extract abstracts\n",
    "        abstracts = df[df['EID'].isin(representative_eids)]['Abstract'].dropna().tolist()\n",
    "        \n",
    "        if abstracts:\n",
    "            # Generate name and description\n",
    "            metadata = generate_cluster_name_description(abstracts, client)\n",
    "            naming_results.append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'name': metadata['name'],\n",
    "                'description': metadata['description']\n",
    "            })\n",
    "        \n",
    "        time.sleep(1.0)  # Rate limiting\n",
    "    \n",
    "    # Apply names to dataframe\n",
    "    naming_df = pd.DataFrame(naming_results)\n",
    "    name_mapping = naming_df.set_index('cluster_id')['name'].to_dict()\n",
    "    desc_mapping = naming_df.set_index('cluster_id')['description'].to_dict()\n",
    "    \n",
    "    df['cluster_name'] = df['5L_Layer_5_Final'].map(name_mapping)\n",
    "    df['cluster_description'] = df['5L_Layer_5_Final'].map(desc_mapping)\n",
    "    \n",
    "    print(f\"‚úÖ Generated names for {len(naming_results)} clusters\")\n",
    "    return df\n",
    "\n",
    "# Generate cluster names\n",
    "df = process_cluster_naming(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete results\n",
    "df.to_csv('final_clustered_papers_with_names.csv', index=False)\n",
    "df.to_pickle('final_clustered_papers_with_names.pkl')\n",
    "\n",
    "# Generate summary statistics\n",
    "cluster_stats = df.groupby('cluster_name').agg({\n",
    "    'EID': 'count',\n",
    "    'density_score': 'mean',\n",
    "    'cluster_description': 'first'\n",
    "}).rename(columns={'EID': 'paper_count'}).sort_values('paper_count', ascending=False)\n",
    "\n",
    "print(\"üìä Final Clustering Summary:\")\n",
    "print(f\"   üìÑ Total papers processed: {len(df)}\")\n",
    "print(f\"   üè∑Ô∏è  Named clusters: {df['cluster_name'].nunique()}\")\n",
    "print(f\"   üìà Largest cluster: {cluster_stats['paper_count'].max()} papers\")\n",
    "print(f\"   üìâ Average cluster size: {cluster_stats['paper_count'].mean():.1f} papers\")\n",
    "\n",
    "# Save cluster statistics\n",
    "cluster_stats.to_csv('cluster_statistics.csv')\n",
    "print(\"\\n‚úÖ All files saved successfully\")\n",
    "print(\"üìÅ Output files:\")\n",
    "print(\"   - final_clustered_papers_with_names.csv\")\n",
    "print(\"   - final_clustered_papers_with_names.pkl\")\n",
    "print(\"   - cluster_statistics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
